{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Autoencoders</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation\n",
    "    \n",
    "requirement.txt can be found in the repository. In order install all necessary packages you should run:\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "Adding Calina library to pythonpath is also necessary. If you are using using anaconda you can just type:\n",
    "\n",
    "conda develop Calina_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from datetime import datetime\n",
    "datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import neptune\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### networks.py contains our custom architectures for autoencoders. They are built upon the pl.LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import VeloDecoder, VeloEncoder, VeloAutoencoderLt\n",
    "from calibration_dataset import Tell1Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are all the necessary parameters for the training process, creating plots and saving them in Neptune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainig parameters\n",
    "PARAMS = {'max_epochs': 1,\n",
    "          'learning_rate': 0.02,\n",
    "          'batch_size': 64,\n",
    "          'gpus': 1,\n",
    "          #'experiment_name': 'small-net more-epochs standarized SGD no-dropout bigger-batches relu shuffle',\n",
    "          'experiment_name': 'test',\n",
    "          #'tags': ['small-net', 'more-epochs', 'standarized', 'SGD', 'no-dropout', 'bigger-batches', 'relu', 'shuffle'],\n",
    "          'tags': ['test'],\n",
    "          'source_files': ['analyze.ipynb', 'networks.py'],\n",
    "          'experiment_id': 'VEL-451'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class for extracting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDS(Tell1Dataset):\n",
    "    filename_format = '%Y-%m-%d'\n",
    "    filename_regex_format = r'\\d{4}-\\d{2}-\\d{2}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data from all sources and channels. We also standardize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:04<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      channel0  channel1  channel2  channel3  channel4  channel5  channel6  \\\n",
      "2         17.0      11.0      11.0      11.0      11.0      11.0      11.0   \n",
      "5         17.0      11.0      11.0      11.0      11.0      11.0      11.0   \n",
      "8         18.0      11.0      11.0      11.0      11.0      11.0      11.0   \n",
      "11        16.0      11.0      11.0      10.0      10.0      11.0      10.0   \n",
      "14        10.0      11.0      10.0      11.0      11.0      10.0      10.0   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7547      10.0      11.0      11.0      11.0      11.0      11.0      10.0   \n",
      "7550       9.0      11.0      11.0      11.0      11.0      11.0      10.0   \n",
      "7553       9.0      11.0      11.0      11.0      11.0      11.0      10.0   \n",
      "7556      10.0      11.0      11.0      11.0      10.0      11.0      11.0   \n",
      "7559       9.0      11.0      10.0      11.0      11.0      11.0      11.0   \n",
      "\n",
      "      channel7  channel8  channel9  ...  channel2038  channel2039  \\\n",
      "2         11.0      11.0      11.0  ...         13.0         12.0   \n",
      "5         11.0      11.0      11.0  ...         12.0         12.0   \n",
      "8         11.0      11.0      11.0  ...         13.0         13.0   \n",
      "11        11.0      11.0      11.0  ...         13.0         14.0   \n",
      "14        10.0      10.0      10.0  ...         13.0         13.0   \n",
      "...        ...       ...       ...  ...          ...          ...   \n",
      "7547      11.0      10.0      11.0  ...         10.0          9.0   \n",
      "7550      11.0      11.0      11.0  ...         11.0          9.0   \n",
      "7553      11.0      11.0      11.0  ...         10.0         10.0   \n",
      "7556      11.0      11.0      11.0  ...         11.0          9.0   \n",
      "7559      11.0      11.0      11.0  ...         10.0          9.0   \n",
      "\n",
      "      channel2040  channel2041  channel2042  channel2043  channel2044  \\\n",
      "2            12.0         13.0         12.0         13.0         12.0   \n",
      "5            12.0         12.0         13.0         13.0         12.0   \n",
      "8            13.0         13.0         13.0         13.0         13.0   \n",
      "11           14.0         13.0         13.0         13.0         13.0   \n",
      "14           13.0         13.0         13.0         13.0         14.0   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "7547         10.0         12.0         10.0         12.0         10.0   \n",
      "7550         10.0         12.0         11.0         12.0         10.0   \n",
      "7553         10.0         12.0         11.0         12.0         10.0   \n",
      "7556         10.0         12.0         10.0         12.0         10.0   \n",
      "7559         11.0         12.0         10.0         12.0         10.0   \n",
      "\n",
      "      channel2045  channel2046  channel2047  \n",
      "2            12.0         12.0         13.0  \n",
      "5            13.0         12.0         12.0  \n",
      "8            13.0         13.0         13.0  \n",
      "11           13.0         14.0         13.0  \n",
      "14           14.0         13.0         14.0  \n",
      "...           ...          ...          ...  \n",
      "7547         12.0         10.0         12.0  \n",
      "7550         12.0         10.0         12.0  \n",
      "7553         12.0         10.0         12.0  \n",
      "7556         12.0         10.0         12.0  \n",
      "7559         12.0         11.0         12.0  \n",
      "\n",
      "[2520 rows x 2048 columns]\n"
     ]
    }
   ],
   "source": [
    "datapath = os.path.join(\"data\", \"calibrations\")\n",
    "data_list = MyDS.get_filepaths_from_dir(datapath)\n",
    "mds = MyDS(data_list, read=True)\n",
    "\n",
    "dfh = mds.dfh.df.iloc[:, 9:]\n",
    "print(dfh)\n",
    "dfh_r = mds.dfh['R'].df.iloc[:, 9:]\n",
    "dfh_phi = mds.dfh['phi'].df.iloc[:, 9:]\n",
    "dfp = mds.dfp.df.iloc[:, 9:]\n",
    "dfp_r = mds.dfp['R'].df.iloc[:, 9:]\n",
    "dfp_phi = mds.dfp['phi'].df.iloc[:, 9:]\n",
    "\n",
    "dfh_metadata = mds.dfh.df.iloc[:, :9]\n",
    "dfh_r_metadata = mds.dfh['R'].df.iloc[:, :9]\n",
    "dfh_phi_metadata = mds.dfh['phi'].df.iloc[:, :9]\n",
    "dfp_metadata = mds.dfp.df.iloc[:, :9]\n",
    "dfp_r_metadata = mds.dfp['R'].df.iloc[:, :9]\n",
    "dfp_phi_metadata = mds.dfp['phi'].df.iloc[:, :9]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "dfh_scaled = scaler.fit_transform(dfh)\n",
    "dfh_r_scaled = scaler.fit_transform(dfh_r)\n",
    "dfh_phi_scaled = scaler.fit_transform(dfh_phi)\n",
    "dfp_scaled = scaler.fit_transform(dfp)\n",
    "dfp_r_scaled = scaler.fit_transform(dfp_r)\n",
    "dfp_phi_scaled = scaler.fit_transform(dfp_phi)\n",
    "\n",
    "dfh = pd.DataFrame(dfh_scaled, index=dfh.index, columns=dfh.columns)\n",
    "dfh_r = pd.DataFrame(dfh_r_scaled, index=dfh_r.index, columns=dfh_r.columns)\n",
    "dfh_phi = pd.DataFrame(dfh_phi_scaled, index=dfh_phi.index, columns=dfh_phi.columns)\n",
    "dfp = pd.DataFrame(dfp_scaled, index=dfp.index, columns=dfp.columns)\n",
    "dfp_r = pd.DataFrame(dfp_r_scaled, index=dfp_r.index, columns=dfp_r.columns)\n",
    "dfp_phi = pd.DataFrame(dfp_phi_scaled, index=dfp_phi.index, columns=dfp_phi.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating loaders for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(dataset):\n",
    "    train, test = train_test_split(dataset, test_size=0.2)\n",
    "    train_target = torch.tensor(train.values, dtype=torch.float)\n",
    "    train_data = torch.tensor(train.values, dtype=torch.float)\n",
    "    test_target = torch.tensor(test.values, dtype=torch.float)\n",
    "    test_data = torch.tensor(test.values, dtype=torch.float)\n",
    "    train_tensor = TensorDataset(train_data, train_target)\n",
    "    test_tensor = TensorDataset(test_data, test_target)\n",
    "    train_loader = DataLoader(dataset=train_tensor, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_tensor, shuffle=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating trainers for the network, with some set training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_trainer(s, neptune_logger, lr):\n",
    "    #s = 2048\n",
    "    dec = VeloDecoder(s)\n",
    "    enc = VeloEncoder(s)\n",
    "    model = VeloAutoencoderLt(enc, dec, lr)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    tr = pl.Trainer(logger=neptune_logger, callbacks=[lr_monitor],  max_epochs=PARAMS['max_epochs'],\n",
    "                    gpus=PARAMS['gpus'])\n",
    "    return model, tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function creating the slider plot to see the evolution of autoencoder results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slider_plot(dataset, datasetName, metadata, model):\n",
    "    reducedData = model.enc.forward(torch.tensor(dataset.values, dtype=torch.float))\n",
    "    reducedData = reducedData.detach().numpy()\n",
    "     \n",
    "    indexesList = metadata.index.values.tolist()\n",
    "    xyDF = pd.DataFrame(reducedData, index=indexesList, columns=['x', 'y'])\n",
    "    resultDF = pd.concat([metadata, xyDF], axis=1)\n",
    "    resultDF[\"datetime\"] = resultDF[\"datetime\"].astype(str)\n",
    "\n",
    "    fig = px.scatter(resultDF, x=\"x\", y=\"y\", animation_frame=\"datetime\", animation_group=\"sensor\", color=\"sensor\")\n",
    "    fig[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\n",
    "    fig.update_xaxes(range=[1.15*resultDF['x'].min(), 1.15*resultDF['y'].max()])\n",
    "    fig.update_yaxes(range=[1.15*resultDF['x'].min(), 1.15*resultDF['y'].max()])\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function creating the plot being the result of the training. All the channels can be later turned on/off in a custom way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_plot(dataset, datasetName, metadata, model):\n",
    "    \n",
    "    reducedData = model.enc.forward(torch.tensor(dataset.values, dtype=torch.float))\n",
    "    reducedData = reducedData.detach().numpy()\n",
    "     \n",
    "    indexesList = metadata.index.values.tolist()\n",
    "    xyDF = pd.DataFrame(reducedData, index=indexesList, columns=['x', 'y'])\n",
    "    resultDF = pd.concat([metadata, xyDF], axis=1)\n",
    "    resultDF[\"datetime\"] = resultDF[\"datetime\"].astype(str)\n",
    "    resultDF[\"sensor\"] = resultDF[\"sensor\"].astype(str)\n",
    "    print(resultDF)\n",
    "    \n",
    "    fig = px.scatter(resultDF, x=\"x\", y=\"y\", color='sensor', opacity=0.5)\n",
    "    fig.show(renderer=\"notebook\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function connects to Neptune, starts a new experiment there and starts running the training process for the network. It then creates all the plots and saves them in the Neptune as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset, datasetName, par, metadata):\n",
    "    model_path = os.path.join('models', par['experiment_name'], datasetName)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    train_loader, test_loader = make_loader(dataset)\n",
    "    s = dataset.shape[1]\n",
    "    neptune_logger = NeptuneLogger(\n",
    "        api_key=os.getenv('NEPTUNE_API_TOKEN'),\n",
    "        project_name=\"pawel-drabczyk/velodimred\",\n",
    "        experiment_name=par['experiment_name'],\n",
    "        params=par,\n",
    "        tags=par['tags'] + [datasetName] + ['interactive'],\n",
    "        upload_source_files=par['source_files']\n",
    "    )\n",
    "    model, tr = make_model_trainer(s, neptune_logger, par['learning_rate'])\n",
    "    tr.fit(model, train_loader, test_loader)\n",
    "\n",
    "    torch.save(model, os.path.join('models', PARAMS['experiment_name'], datasetName, \"trained_model.ckpt\"))\n",
    "    neptune_logger.experiment.log_artifact(os.path.join(model_path, \"trained_model.ckpt\"))\n",
    "    \n",
    "    fig = slider_plot(dataset, datasetName, metadata, model)\n",
    "    fig.write_html(os.path.join(model_path, 'slider_plot.html'))\n",
    "    fig.write_image(os.path.join(model_path, 'slider_plot.png'))   \n",
    "    neptune_logger.experiment.log_image('slider_plot',os.path.join(model_path, 'slider_plot.png'))    \n",
    "    fig = clustering_plot(dataset, datasetName, metadata, model)\n",
    "    fig.write_html(os.path.join(model_path, 'clustering_plot.html'))\n",
    "    fig.write_image(os.path.join(model_path, 'clustering_plot.png'))    \n",
    "    neptune_logger.experiment.log_image('clustering_plot', os.path.join(model_path, 'clustering_plot.png'))        \n",
    "    \n",
    "    neptune_logger.experiment.log_artifact(os.path.join(model_path, \"slider_plot.html\"))\n",
    "    neptune_logger.experiment.log_artifact(os.path.join(model_path, \"clustering_plot.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function was used to reopen experiments, and add the cluster plots and slider plots, which was added at the end of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reopen_experiment(dataset, datasetName, par, metadata):\n",
    "    exp_id = par['experiment_id']\n",
    "    exp_name = par['experiment_name']\n",
    "    model_path = os.path.join('models', exp_name, datasetName)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    \n",
    "    project = neptune.init('pawel-drabczyk/velodimred')\n",
    "    my_exp = project.get_experiments(id=exp_id)[0]\n",
    "    my_exp.download_artifact('trained_model.ckpt', model_path)\n",
    "    my_exp.download_sources('networks.py')\n",
    "    with zipfile.ZipFile(\"networks.py.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    from networks import VeloDecoder, VeloEncoder, VeloAutoencoderLt\n",
    "        \n",
    "    model = torch.load(os.path.join(model_path,'trained_model.ckpt'))\n",
    "    \n",
    "    fig = slider_plot(dataset, datasetName, metadata, model)\n",
    "    fig.write_html(os.path.join(model_path, 'slider_plot.html'))\n",
    "    fig.write_image(os.path.join(model_path, 'slider_plot.png'))   \n",
    "    my_exp.log_image('slider_plot',os.path.join(model_path, 'slider_plot.png'))    \n",
    "    fig = clustering_plot(dataset, datasetName, metadata, model)\n",
    "    fig.write_html(os.path.join(model_path, 'clustering_plot.html'))\n",
    "    fig.write_image(os.path.join(model_path, 'clustering_plot.png'))    \n",
    "    my_exp.log_image('clustering_plot', os.path.join(model_path, 'clustering_plot.png'))        \n",
    "    \n",
    "    my_exp.log_artifact(os.path.join(model_path, \"slider_plot.html\"))\n",
    "    my_exp.log_artifact(os.path.join(model_path, \"clustering_plot.html\"))\n",
    "    my_exp.append_tag('interactive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell below runs te training process for all the datasets, for the current experiment, for the current network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetNames = ['dfh', 'dfhr', 'dfhphi', 'dfp', 'dfpr', 'dfpphi']\n",
    "\n",
    "#run_experiment(dfh, 'dfh', PARAMS, dfh_metadata)\n",
    "#run_experiment(dfh_r, 'dfhr', PARAMS, dfh_r_metadata)\n",
    "#run_experiment(dfh_phi, 'dfhphi', PARAMS, dfh_phi_metadata)\n",
    "#run_experiment(dfp, 'dfp', PARAMS, dfp_metadata)\n",
    "#run_experiment(dfp_r, 'dfpr', PARAMS, dfp_r_metadata)\n",
    "#run_experiment(dfp_phi, 'dfpphi', PARAMS, dfp_phi_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell below opens existing experiment, adding the slider plots and cluster plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "DownloadSourcesException",
     "evalue": "\n\u001b[95m\n----DownloadSourcesException-----------------------------------------------------------------------\n\u001b[0m\nNeptune Client Library was not able to download single file from sources.\n\nWhy am I seeing this?\n    Your project \"751f089a-43fb-4bb2-9e5c-1792690bfd6d\" has been migrated to new structure.\n    Old version of `neptune-api` is not supporting downloading particular source files.\n    We recommend you to use new version of api: `neptune.new`.\n    \u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n\nIf you don't want to adapt your code to new api yet,\nyou can use `download_sources` with `path` parameter set to None.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDownloadSourcesException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-adc5b3e1a52e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reopen_experiment(dfh, 'dfh', PARAMS, dfh_metadata)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreopen_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfh_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dfhr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfh_r_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#reopen_experiment(dfh_phi, 'dfhphi', PARAMS, dfh_phi_metadata)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#reopen_experiment(dfp, 'dfp', PARAMS, dfp_metadata)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#reopen_experiment(dfp_r, 'dfpr', PARAMS, dfp_r_metadata)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-447fb464e4ca>\u001b[0m in \u001b[0;36mreopen_experiment\u001b[0;34m(dataset, datasetName, par, metadata)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmy_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmy_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained_model.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmy_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'networks.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"networks.py.zip\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LHCb/lib/python3.8/site-packages/neptune/experiments.py\u001b[0m in \u001b[0;36mdownload_sources\u001b[0;34m(self, path, destination_dir)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'src/my-module'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sources/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \"\"\"\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LHCb/lib/python3.8/site-packages/neptune/internal/api_clients/hosted_api_clients/hosted_alpha_leaderboard_api_client.py\u001b[0m in \u001b[0;36mdownload_sources\u001b[0;34m(self, experiment, path, destination_dir)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# in alpha all source files stored as single FileSet must be downloaded at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDownloadSourcesException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_consts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOURCE_CODE_FILES_ATTRIBUTE_PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDownloadSourcesException\u001b[0m: \n\u001b[95m\n----DownloadSourcesException-----------------------------------------------------------------------\n\u001b[0m\nNeptune Client Library was not able to download single file from sources.\n\nWhy am I seeing this?\n    Your project \"751f089a-43fb-4bb2-9e5c-1792690bfd6d\" has been migrated to new structure.\n    Old version of `neptune-api` is not supporting downloading particular source files.\n    We recommend you to use new version of api: `neptune.new`.\n    \u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n\nIf you don't want to adapt your code to new api yet,\nyou can use `download_sources` with `path` parameter set to None.\n"
     ]
    }
   ],
   "source": [
    "#reopen_experiment(dfh, 'dfh', PARAMS, dfh_metadata)\n",
    "reopen_experiment(dfh_r, 'dfhr', PARAMS, dfh_r_metadata)\n",
    "#reopen_experiment(dfh_phi, 'dfhphi', PARAMS, dfh_phi_metadata)\n",
    "#reopen_experiment(dfp, 'dfp', PARAMS, dfp_metadata)\n",
    "#reopen_experiment(dfp_r, 'dfpr', PARAMS, dfp_r_metadata)\n",
    "#reopen_experiment(dfp_phi, 'dfpphi', PARAMS, dfp_phi_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
